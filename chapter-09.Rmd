# A Summary of Solubility Models


```{r chapter-09-startup, include = FALSE}
knitr::opts_chunk$set(fig.path = "figures/")
library(knitr)
library(tidymodels)
library(workflowsets)

caching <- TRUE

cores <- parallel::detectCores()
if (!grepl("mingw32", R.Version()$platform)) {
 library(doMC)
 registerDoMC(cores = cores)
} else {
  library(doParallel)
  cl <- makePSOCKcluster(cores)
  registerDoParallel(cl)
}

# ------------------------------------------------------------------------------

load("RData/chapter_06.RData")
load("RData/chapter_07.RData")
load("RData/chapter_08.RData")

# obj <- sort(ls(pattern = "(_tune$)|(_resamp$)|(_bo$)"))
# obj_name <- strsplit(obj, "_") %>% purrr::map_chr(~ .x[1])
# obj_name[obj  == "svm_r_tune"] <- "svm (radial)"
# obj_name[obj  == "svm_p_tune"] <- "svm (poly)"
# obj_name[obj  == "lin"] <- "ols"
# obj_name[obj  == "reg"] <- "model rules"
# paste0("'", obj_name, "' = ", obj, collapse = ", ")
```

In _APM_, this chapter gathers the results across all of the models created in Chapters 6, 7, and 8. It compares their performance metrics. We'll do the same here and introduce another R package. The R packages used in this chapter are: `r pkg_text(c("tidymodels", "rules", "Cubist", "workflowsets", "patchwork"))`. 

The `r pkg(workflowsets)` package enables a method for collecting workflow into a _workflow set_. For our previous results, we can collect the objects with the results into a list then create a workflow set:

```{r chapter-09-collect}
library(tidymodels)
library(workflowsets)

solubility_res <- 
  as_workflow_set(
    'bag' = bag_cart_resamp, 'cart' = cart_tune, 'cubist' = cubist_tune, 
    'glmnet' = glmnet_tune, 'knn' = knn_tune, 'lasso' = lasso_tune, 
    'ols' = lin_reg_tune, 'mars' = mars_tune, 'nnet' = nnet_bo, 
    'pcr' = pcr_tune, 'pls' = pls_tune, 'model rules' = reg_rules_tune, 
    'rf' = rf_tune, 'ridge' = ridge_tune, 'rulefit' = rulefit_bo, 
    'svm (poly)' = svm_p_tune, 'svm (radial)' = svm_r_tune, 'xgb' = xgb_bo
  )
solubility_res
```

The next chapter shows the real power of workflow sets: creating and evaluating them _en masse_. 

The `r pkg(workflowsets)` package has some nice summary methods. `rank_results()` will list the model showing the best configurations based on a single metric. We only used RMSE here so there is no need to specify the ranking metric.

```{r chapter-09-summarize}
rank_results(solubility_res)
```
The shows _everything_. A more succinct summary comes from using `select_best = TRUE` so that a single tuning parameter combination is shown for each model: 

```{r chapter-09-summarize-best}
rank_results(solubility_res, select_best = TRUE)
```

The `autoplot()` function also provides a nice visualization of the results:

```{r chapter-09-summarize-plot}
autoplot(solubility_res, select_best = TRUE) +
 theme(legend.position = "right")
```

This produces 90% confidence intervals for the RMSE estimates. Alternatively, a more statistically sophisticated approach to comparing these models is to use the `r pkg(tidyposterior)` package. It fits a Bayesian ANOVA model to the RMSE estimates from the resampling process and can make more formal probabilistic comparisons between models. The `perf_mod()` function, when applied to workflow sets, takes the best submodel from each workflow. To make the results a little more conservative we'll use a prior (a 1-df _t_) with heavier tails than the default Gaussian. 


```{r chapter-09-tidy-post}
library(tidyposterior)
rmse_mod <-
 perf_mod(
  solubility_res,
  prior_intercept = rstanarm::student_t(1),
  # MCMC parameters:
  chains = 8,
  cores = 8,
  iter = 5000,
  seed = 1,
  # Don't print excessive output
  refresh = 0
 )
```

The regular `autoplot()` method produces 90% _credible intervals_. These are intervals that have a 90% probability of containing the true RMSE for the model. The results are fairly similar to the previous analysis: 

```{r chapter-09-tidy-post-plot}
autoplot(rmse_mod) 
```

One nice thing about the Bayesian analysis is that we can compute some interesting probability estimates. For example, suppose we believe that, irregardless of these results, a difference of 0.1 RMSE units would be a real/important difference between models. From this user-defined value, we can compute the probability that the model performance is equal to the best model in the workflow set: 

```{r chapter-09-tidy-post-rope}
autoplot(rmse_mod, type = "ROPE", size = 0.1) 
```

Arguably, the first 2-4 models have effectively equal performance based on this practical effect size. 

Suppose we settled on a Cubist model. The next steps would be to

 1. Select the final tuning parameters.
 1. Fit the final model on the entire training set. 
 1. Predict and evaluate the test set. 

In tidymodels, we've already seen the `finalize_workflow()` function. We can finalize on any tuning parameters by passing in a tibble with those values. Here, we will finalize on the numerically best result so that the workflow has real values (instead of the `tune()` placeholders):

```{r chapter-09-final-cubist}
library(rules)

cubist_final_wflow <- 
 cubist_wflow %>% 
 finalize_workflow(select_best(cubist_tune))
cubist_final_wflow
```

To fit the final model and evaluate the test set, there is a convenience function called `last_fit()` that uses the initial training/testing split object produced by the `r pkg(rsample)` package. It fits the workflow to the training set, saves the fitted model, and returns the test set metrics and predictions. To get the split object, we load the original data objects:

```{r chapter-09-last-fit, message = FALSE}
load("solubility_data.RData")

cubist_final_res <- 
 cubist_final_wflow %>% 
 last_fit(split = solubility_split)

cubist_final_res
```

The `.workflow` column contains the fitted recipe and model from the training set. The `.metrics` and `.predictions` columns contain the appropriate test set values but the usual accessor functions can also be used: 

```{r chapter-09-last-fit-metrics, message = FALSE}
test_results <- collect_metrics(cubist_final_res)
test_results

# Resampling results:
show_best(cubist_tune, n = 1)
```

The test set results look slightly worse than the training set statistics (but this is consistent with the results shown in _APM_). 

The `collect_predictions()` function can also be used to get a data frame of predicted values. Our `regression_plots()` function will also generate the standard plot (although the test set results are shown here):

```{r chapter-09-last-fit-plot}
test_pred <- collect_predictions(cubist_final_res)
test_pred

regression_plots(cubist_final_res)
```

